{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bRPN2-9luXnYH0cyOl3DFjyOdCkXkGGk",
      "authorship_tag": "ABX9TyPdL/tQVCy+u2RI+Q6uaAz8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archiegoodman2/neural_net/blob/main/models_UNSW_NB15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice run of analysing/testing different models on the UNSW_NB15 dataset, before trying Deep Learning.\n",
        "\n",
        "Prior research suggests this is a largely non-linear, less separable dataset so deep learning may be necessary, but I will try simpler, more interpretable models first for the sake of completeness, and to gain Variable Importances"
      ],
      "metadata": {
        "id": "Yhq0AM-GxTw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load our packages and data"
      ],
      "metadata": {
        "id": "BwpZ83SO1wod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import packages:\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"New run: Packages loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSMa2bmU5XCI",
        "outputId": "20ef5fcb-8fab-4f39-8aea-5df2350f8865"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New run: Packages loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#if using colabs - will need to first mount your drive\n",
        "\n",
        "#change these for different users\n",
        "test_set_filepath = '/content/drive/MyDrive/Colab_Notebooks/Data/UNSW_NB15_testing-set.parquet'\n",
        "training_set_filepath = '/content/drive/MyDrive/Colab_Notebooks/Data/UNSW_NB15_training-set.parquet'\n",
        "\n",
        "# Import the two CSV files\n",
        "test_set = pd.read_parquet(test_set_filepath)\n",
        "train_set = pd.read_parquet(training_set_filepath)\n",
        "\n"
      ],
      "metadata": {
        "id": "OsQcbrhZyWcN"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next cell does some basic analysis, and one hot encodes some of the features:"
      ],
      "metadata": {
        "id": "N7RupKUW7OgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#print number of records in our data\n",
        "print(f\"Number of records in training set: {len(train_set)}\")\n",
        "print(f\"Number of records in test set: {len(test_set)}\")\n",
        "\n",
        "#lets see which ones are categorical etc\n",
        "print(f'''\n",
        "The columns and datatypes are:\n",
        "{train_set.dtypes}\n",
        "''')\n",
        "\n",
        "print(\"Categorical Columns are :\", categorical_cols)\n",
        "\n",
        "#print out number of classifications\n",
        "print(f\"Number of categories in 'label' category: {len(train_set['label'].unique())}\")\n",
        "\n",
        "#print out labels\n",
        "print(f\"Labels: {train_set['label'].unique()}\")\n",
        "\n",
        "#print out how many unique values we have for each categorical variable - if we have too many we may need an embeddings layer\n",
        "for col in categorical_cols:\n",
        "    print(f\"Number of categories in '{col}' category: {len(train_set[col].unique())}\")\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM4RjBPs6w0F",
        "outputId": "b63a6145-f67c-4a27-a08e-be74c78f86f5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of records in training set: 175341\n",
            "Number of records in test set: 82332\n",
            "Categorical Columns are : ['proto', 'service', 'state', 'attack_cat']\n",
            "Number of categories in 'label' category: 2\n",
            "Labels: [0 1]\n",
            "Number of categories in 'proto' category: 133\n",
            "Number of categories in 'service' category: 13\n",
            "Number of categories in 'state' category: 9\n",
            "Number of categories in 'attack_cat' category: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_data(data_set):\n",
        "  \"\"\"\n",
        "  Function to preprocess data. One hot encodes the top 6 most common values for 'proto'.\n",
        "  And turns boolean columns into 1s and 0s.\n",
        "\n",
        "  Args:\n",
        "  data_set (dataframe) : test or train set to be processed\n",
        "\n",
        "  Retuns:\n",
        "  data_set (dataframe) : processed data set\n",
        "\n",
        "  \"\"\"\n",
        "  # List only the categorical columns (object types)\n",
        "  categorical_cols = data_set.select_dtypes(include=['category']).columns.tolist()\n",
        "\n",
        "  #there seems to be over 100 possible values of proto - lets see how common they all are\n",
        "  category_percentages = data_set['proto'].value_counts(normalize=True) * 100\n",
        "\n",
        "  #define a dict of the categories and their percentages of occurence. what we want to do here is group any that occur less than 0.5% of the time, into an 'other' category\n",
        "  category_percentages_dict = category_percentages.to_dict()\n",
        "  #we can then print this to view the distributions ^\n",
        "\n",
        "  # After looking at the distributions of the possible values for Proto, only the top 6 occur more than 0.5% of the time - hence all others are very rare\n",
        "  # So we get the top 6 most common values. We have to hardcode in this value of top 6\n",
        "  top_6_categories = category_percentages.head(6).index.tolist()\n",
        "\n",
        "  #we now have a list of values that we want to one hot encode. we want to simply group the others into an 'other column'\n",
        "  data_set['proto_grouped'] = data_set['proto'].apply(lambda x: x if x in top_6_categories else 'other')\n",
        "\n",
        "  #now we one hot encode this column\n",
        "  data_set = pd.get_dummies(data_set, columns=['proto_grouped'])\n",
        "\n",
        "  #drop the original columns if still present\n",
        "  if 'proto' in data_set.columns:\n",
        "    data_set = data_set.drop('proto', axis=1)\n",
        "  if 'proto_grouped' in data_set.columns:\n",
        "      data_set = data_set.drop(['proto_grouped'], axis=1)\n",
        "\n",
        "  #encode all binary data as 1s and 0s\n",
        "  binary_cols = data_set.select_dtypes(include=['bool']).columns\n",
        "\n",
        "  #convert to int - the original apply function was returning a dataframe instead of a series in cases where more than 1 value was found. This is because the column has more than 1 unique value. This can happen when for example, you expect all the values to be `True` but then find some are `False` as well. in that scenario the apply function will not collapse this down to a series.\n",
        "  data_set[binary_cols] = data_set[binary_cols].apply(lambda x: x.astype(int))\n",
        "\n",
        "  return data_set\n",
        "\n",
        "train_set = preprocess_data(train_set)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VengASbQCV3w"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMRXGNRqMAaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE TO SELF -\n",
        "1. optimise lambda funct for data preprocessing?\n",
        "2. do i have to also preprocess test set"
      ],
      "metadata": {
        "id": "eFYJj2gn1rqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the high number of columns in the Proto column, we may want to consider an Embeddings layer with the Deep Learning that we plan to undertake later. However since DT/RF perform somewhat poorly on sparse vector datasets (like one hot encoded ones) we will group all the extremely rare categories into an 'other'.\n"
      ],
      "metadata": {
        "id": "sYPGA0w_tgSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_models(train_set, model_type):\n",
        "  \"\"\"\n",
        "  Runs LR, DT or RF model on dataframe\n",
        "  \"\"\"\n",
        "\n",
        "  train_set = preprocess_data(train_set)\n",
        "\n",
        "  #drop label and define list of out targets\n",
        "  X = train_set.drop('label', axis=1)\n",
        "  y = train_set['label']\n",
        "\n",
        "  # List only the categorical columns (object types)\n",
        "  categorical_cols = train_set.select_dtypes(include=['category']).columns.tolist()\n",
        "\n",
        "  #we plan to use nested k fold cross validation for the HPs so let's define a dict of lists containing sensible guesses for the HPs for our three models\n",
        "  param_grid_lr = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10],   # List of regularization strengths\n",
        "    'solver': ['lbfgs', 'saga']         # List of solvers to try\n",
        "  }\n",
        "\n",
        "  param_grid_dt = {\n",
        "    'max_depth': [3, 5, 10],             # List of max depth values\n",
        "    'min_samples_split': [2, 10, 20],    # List of minimum samples to split\n",
        "    'min_samples_leaf': [1, 5, 10]       # List of minimum samples per leaf\n",
        "  }\n",
        "\n",
        "  param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],      # List of number of trees to use\n",
        "    'max_depth': [5, 10, 20],            # List of maximum depths for trees\n",
        "    'min_samples_split': [2, 10, 20]     # List of minimum samples for splitting a node\n",
        "  }\n",
        "\n",
        "  if model_type == 'LR':\n",
        "\n",
        "    # Standardize features (for Logistic Regression)\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    #init our model\n",
        "    LR_model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "  if model_type == 'DT':\n",
        "\n",
        "    DT_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "  if model_type == 'RF':\n",
        "\n",
        "    RF_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqRkXuXnMjmJ"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}